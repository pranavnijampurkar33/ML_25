{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparing to Regular Vector Database Retrieval\n",
    "\n",
    "<img src=\"./media/basic_retrieval.png\" width=600>\n",
    " \n",
    "To give some comparison, let's look back at traditional chunking, embedding, and similarity retrieval RAG\n",
    "\n",
    "<img src=\"./media/RAG_QA.webp\" width=600 style=\"background-color: white;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate our Database**\n",
    "\n",
    "For this we'll be using [ChromaDB](https://www.trychroma.com) with the same chunks as were loaded into our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embed Chunks Into Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG Prompt & Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion\n",
    "\n",
    "**Traditional/Naive RAG:**\n",
    "\n",
    "Benefits:\n",
    "- Simpler implementation and deployment\n",
    "- Works well for straightforward information retrieval tasks\n",
    "- Good at handling unstructured text data\n",
    "- Lower computational overhead\n",
    "\n",
    "Drawbacks:\n",
    "- Loses structural information when chunking documents\n",
    "- Can break up related content during text segmentation\n",
    "- Limited ability to capture relationships between different pieces of information\n",
    "- May struggle with complex reasoning tasks requiring connecting multiple facts\n",
    "- Potential for incomplete or fragmented answers due to chunking boundaries\n",
    "\n",
    "**GraphRAG:**\n",
    "\n",
    "Benefits:\n",
    "- Preserves structural relationships and hierarchies in the knowledge\n",
    "- Better at capturing connections between related information\n",
    "- Can provide more complete and contextual answers\n",
    "- Improved retrieval accuracy by leveraging graph structure\n",
    "- Better supports complex reasoning across multiple facts\n",
    "- Can maintain document coherence better than chunk-based approaches\n",
    "- More interpretable due to explicit knowledge representation\n",
    "\n",
    "Drawbacks:\n",
    "- More complex to implement and maintain\n",
    "- Requires additional processing to construct and update knowledge graphs\n",
    "- Higher computational overhead for graph operations\n",
    "- May require domain expertise to define graph schema/structure\n",
    "- More challenging to scale to very large datasets\n",
    "- Additional storage requirements for graph structure\n",
    "\n",
    "**Key Differentiators:**\n",
    "1. Knowledge Representation: Traditional RAG treats everything as flat text chunks, while GraphRAG maintains structured relationships in a graph format\n",
    "\n",
    "2. Context Preservation: GraphRAG better preserves context and relationships between different pieces of information compared to the chunking approach of traditional RAG\n",
    "\n",
    "3. Reasoning Capability: GraphRAG enables better multi-hop reasoning and connection of related facts through graph traversal, while traditional RAG is more limited to direct retrieval\n",
    "\n",
    "4. Answer Quality: GraphRAG tends to produce more complete and coherent answers since it can access related information through graph connections rather than being limited by chunk boundaries\n",
    "\n",
    "The choice between traditional RAG and GraphRAG often depends on the specific use case, with GraphRAG being particularly valuable when maintaining relationships between information is important or when complex reasoning is required. An important note as well, GraphRAG approaches still rely on regular embedding and retrieval methods themselves. They compliment eahcother!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (0.12.30)\n",
      "Requirement already satisfied: graspologic in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: numpy==1.24.4 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (1.24.4)\n",
      "Requirement already satisfied: scipy==1.12.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (0.4.6)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.30 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (0.12.30)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (0.6.11)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (0.3.33)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (0.4.7)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: anytree>=2.8.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (2.13.0)\n",
      "Requirement already satisfied: beartype>=0.10.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (0.20.2)\n",
      "Requirement already satisfied: gensim!=4.2.0,>=4.0.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (4.3.3)\n",
      "Requirement already satisfied: graspologic-native>=1.1.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (1.2.5)\n",
      "Requirement already satisfied: hyppo>=0.3.2 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (0.5.1)\n",
      "Requirement already satisfied: joblib>=0.17.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (1.4.2)\n",
      "Requirement already satisfied: matplotlib!=3.3.*,!=3.6.1,>=3.0.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (3.10.1)\n",
      "Requirement already satisfied: networkx>=2.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (3.4.2)\n",
      "Requirement already satisfied: POT>=0.7.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (0.9.5)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (1.6.1)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (0.14.4)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (4.13.2)\n",
      "Requirement already satisfied: umap-learn>=0.4.6 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from graspologic) (0.5.7)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from gensim!=4.2.0,>=4.0.0->graspologic) (7.1.0)\n",
      "Requirement already satisfied: numba>=0.46 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from hyppo>=0.3.2->graspologic) (0.61.2)\n",
      "Requirement already satisfied: autograd>=1.3 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from hyppo>=0.3.2->graspologic) (1.7.0)\n",
      "Requirement already satisfied: openai>=1.14.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.73.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.30->llama-index) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (3.9.5)\n",
      "Requirement already satisfied: banks<3.0.0,>=2.0.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (2.1.1)\n",
      "Requirement already satisfied: dataclasses-json in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (0.6.6)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (2025.3.2)\n",
      "Requirement already satisfied: httpx in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.6.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (11.2.1)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (2.11.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (4.66.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.17.2)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.18)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pandas in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.4.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.12)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from matplotlib!=3.3.*,!=3.6.1,>=3.0.0->graspologic) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from matplotlib!=3.3.*,!=3.6.1,>=3.0.0->graspologic) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from matplotlib!=3.3.*,!=3.6.1,>=3.0.0->graspologic) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from matplotlib!=3.3.*,!=3.6.1,>=3.0.0->graspologic) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from matplotlib!=3.3.*,!=3.6.1,>=3.0.0->graspologic) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from matplotlib!=3.3.*,!=3.6.1,>=3.0.0->graspologic) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from matplotlib!=3.3.*,!=3.6.1,>=3.0.0->graspologic) (2.9.0)\n",
      "Requirement already satisfied: click in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (2024.5.15)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from scikit-learn>=0.22.0->graspologic) (3.6.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from statsmodels>=0.13.2->graspologic) (1.0.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from umap-learn>=0.4.6->graspologic) (0.5.13)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (4.0.3)\n",
      "Requirement already satisfied: griffe in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.7.2)\n",
      "Requirement already satisfied: jinja2 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.1.6)\n",
      "Requirement already satisfied: platformdirs in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (4.3.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
      "Requirement already satisfied: anyio in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (0.14.0)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.12 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.12)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from numba>=0.46->hyppo>=0.3.2->graspologic) (0.44.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.3.*,!=3.6.1,>=3.0.0->graspologic) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.21.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.2.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from llama-cloud-services>=0.6.12->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
      "Requirement already satisfied: colorama>=0.4 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index graspologic numpy==1.24.4 scipy==1.12.0 future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_index.core import Document\n",
    "\n",
    "# Load sample dataset\n",
    "news = pd.read_csv(\"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\")[:50]\n",
    "\n",
    "# Convert data into LlamaIndex Document objects\n",
    "documents = [\n",
    "    Document(text=f\"{row['title']}: {row['text']}\")\n",
    "    for _, row in news.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "entity_pattern = r'entity_name:\\s*(.+?)\\s*entity_type:\\s*(.+?)\\s*entity_description:\\s*(.+?)\\s*'\n",
    "relationship_pattern = r'source_entity:\\s*(.+?)\\s*target_entity:\\s*(.+?)\\s*relation:\\s*(.+?)\\s*relationship_description:\\s*(.+?)\\s*'\n",
    "\n",
    "def parse_fn(response_str: str):\n",
    "    entities = re.findall(entity_pattern, response_str)\n",
    "    relationships = re.findall(relationship_pattern, response_str)\n",
    "    return entities, relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from typing import Any, List, Callable, Optional, Union, Dict\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "from llama_index.core.indices.property_graph.utils import (\n",
    "    default_parse_triplets_fn,\n",
    ")\n",
    "from llama_index.core.graph_stores.types import (\n",
    "    EntityNode,\n",
    "    KG_NODES_KEY,\n",
    "    KG_RELATIONS_KEY,\n",
    "    Relation,\n",
    ")\n",
    "from llama_index.core.llms.llm import LLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.prompts.default_prompts import (\n",
    "    DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n",
    ")\n",
    "from llama_index.core.schema import TransformComponent, BaseNode\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "class GraphRAGExtractor(TransformComponent):\n",
    "    \"\"\"Extract triples from a graph.\n",
    "\n",
    "    Uses an LLM and a simple prompt + output parsing to extract paths (i.e. triples) and entity, relation descriptions from text.\n",
    "\n",
    "    Args:\n",
    "        llm (LLM):\n",
    "            The language model to use.\n",
    "        extract_prompt (Union[str, PromptTemplate]):\n",
    "            The prompt to use for extracting triples.\n",
    "        parse_fn (callable):\n",
    "            A function to parse the output of the language model.\n",
    "        num_workers (int):\n",
    "            The number of workers to use for parallel processing.\n",
    "        max_paths_per_chunk (int):\n",
    "            The maximum number of paths to extract per chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    llm: LLM\n",
    "    extract_prompt: PromptTemplate\n",
    "    parse_fn: Callable\n",
    "    num_workers: int\n",
    "    max_paths_per_chunk: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[LLM] = None,\n",
    "        extract_prompt: Optional[Union[str, PromptTemplate]] = None,\n",
    "        parse_fn: Callable = default_parse_triplets_fn,\n",
    "        max_paths_per_chunk: int = 10,\n",
    "        num_workers: int = 4,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        from llama_index.core import Settings\n",
    "\n",
    "        if isinstance(extract_prompt, str):\n",
    "            extract_prompt = PromptTemplate(extract_prompt)\n",
    "\n",
    "        super().__init__(\n",
    "            llm=llm or Settings.llm,\n",
    "            extract_prompt=extract_prompt or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n",
    "            parse_fn=parse_fn,\n",
    "            num_workers=num_workers,\n",
    "            max_paths_per_chunk=max_paths_per_chunk,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"GraphExtractor\"\n",
    "\n",
    "    def __call__(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes.\"\"\"\n",
    "        return asyncio.run(\n",
    "            self.acall(nodes, show_progress=show_progress, **kwargs)\n",
    "        )\n",
    "\n",
    "    async def _aextract(self, node: BaseNode) -> BaseNode:\n",
    "        \"\"\"Extract triples from a node.\"\"\"\n",
    "        assert hasattr(node, \"text\")\n",
    "\n",
    "        text = node.get_content(metadata_mode=\"llm\")\n",
    "        try:\n",
    "            llm_response = await self.llm.apredict(\n",
    "                self.extract_prompt,\n",
    "                text=text,\n",
    "                max_knowledge_triplets=self.max_paths_per_chunk,\n",
    "            )\n",
    "            entities, entities_relationship = self.parse_fn(llm_response)\n",
    "        except ValueError:\n",
    "            entities = []\n",
    "            entities_relationship = []\n",
    "\n",
    "        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])\n",
    "        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])\n",
    "        metadata = node.metadata.copy()\n",
    "        for entity, entity_type, description in entities:\n",
    "            metadata[\n",
    "                \"entity_description\"\n",
    "            ] = description  # Not used in the current implementation. But will be useful in future work.\n",
    "            entity_node = EntityNode(\n",
    "                name=entity, label=entity_type, properties=metadata\n",
    "            )\n",
    "            existing_nodes.append(entity_node)\n",
    "\n",
    "        metadata = node.metadata.copy()\n",
    "        for triple in entities_relationship:\n",
    "            subj, rel, obj, description = triple\n",
    "            subj_node = EntityNode(name=subj, properties=metadata)\n",
    "            obj_node = EntityNode(name=obj, properties=metadata)\n",
    "            metadata[\"relationship_description\"] = description\n",
    "            rel_node = Relation(\n",
    "                label=rel,\n",
    "                source_id=subj_node.id,\n",
    "                target_id=obj_node.id,\n",
    "                properties=metadata,\n",
    "            )\n",
    "\n",
    "            existing_nodes.extend([subj_node, obj_node])\n",
    "            existing_relations.append(rel_node)\n",
    "\n",
    "        node.metadata[KG_NODES_KEY] = existing_nodes\n",
    "        node.metadata[KG_RELATIONS_KEY] = existing_relations\n",
    "        return node\n",
    "\n",
    "    async def acall(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes async.\"\"\"\n",
    "        jobs = []\n",
    "        for node in nodes:\n",
    "            jobs.append(self._aextract(node))\n",
    "\n",
    "        return await run_jobs(\n",
    "            jobs,\n",
    "            workers=self.num_workers,\n",
    "            show_progress=show_progress,\n",
    "            desc=\"Extracting paths from text\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_TRIPLET_EXTRACT_TMPL = \"\"\"\n",
    "-Goal-\n",
    "Given a text document, identify all entities and their entity types from the text and all relationships among the identified entities.\n",
    "Given the text, extract up to {max_knowledge_triplets} entity-relation triplets.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: Type of the entity\n",
    "- entity_description: Comprehensive description of the entity's attributes and activities\n",
    "Format each entity as (\"entity\")\n",
    "\n",
    "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n",
    "For each pair of related entities, extract the following information:\n",
    "- source_entity: name of the source entity, as identified in step 1\n",
    "- target_entity: name of the target entity, as identified in step 1\n",
    "- relation: relationship between source_entity and target_entity\n",
    "- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n",
    "\n",
    "Format each relationship as (\"relationship\")\n",
    "\n",
    "3. When finished, output.\n",
    "\n",
    "-Real Data-\n",
    "######################\n",
    "text: {text}\n",
    "######################\n",
    "output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extractor = GraphRAGExtractor(\n",
    "    llm=llm,\n",
    "    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,\n",
    "    max_paths_per_chunk=2,\n",
    "    parse_fn=parse_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: future in /Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/u-pc-248/miniconda3/envs/docai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from llama_index.core.graph_stores import SimplePropertyGraphStore\n",
    "import networkx as nx\n",
    "from graspologic.partition import hierarchical_leiden\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "class GraphRAGStore(SimplePropertyGraphStore):\n",
    "    community_summary = {}\n",
    "    max_cluster_size = 5\n",
    "\n",
    "    def generate_community_summary(self, text):\n",
    "        \"\"\"Generate summary for a given text using an LLM.\"\"\"\n",
    "        messages = [\n",
    "            ChatMessage(\n",
    "                role=\"system\",\n",
    "                content=(\n",
    "                    \"You are provided with a set of relationships from a knowledge graph, each represented as \"\n",
    "                    \"entity1->entity2->relation->relationship_description. Your task is to create a summary of these \"\n",
    "                    \"relationships. The summary should include the names of the entities involved and a concise synthesis \"\n",
    "                    \"of the relationship descriptions. The goal is to capture the most critical and relevant details that \"\n",
    "                    \"highlight the nature and significance of each relationship. Ensure that the summary is coherent and \"\n",
    "                    \"integrates the information in a way that emphasizes the key aspects of the relationships.\"\n",
    "                ),\n",
    "            ),\n",
    "            ChatMessage(role=\"user\", content=text),\n",
    "        ]\n",
    "        response = OpenAI().chat(messages)\n",
    "        clean_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return clean_response\n",
    "\n",
    "    def build_communities(self):\n",
    "        \"\"\"Builds communities from the graph and summarizes them.\"\"\"\n",
    "        nx_graph = self._create_nx_graph()\n",
    "        community_hierarchical_clusters = hierarchical_leiden(\n",
    "            nx_graph, max_cluster_size=self.max_cluster_size\n",
    "        )\n",
    "        community_info = self._collect_community_info(\n",
    "            nx_graph, community_hierarchical_clusters\n",
    "        )\n",
    "        self._summarize_communities(community_info)\n",
    "\n",
    "    def _create_nx_graph(self):\n",
    "        \"\"\"Converts internal graph representation to NetworkX graph.\"\"\"\n",
    "        nx_graph = nx.Graph()\n",
    "        for node in self.graph.nodes.values():\n",
    "            nx_graph.add_node(str(node))\n",
    "        for relation in self.graph.relations.values():\n",
    "            nx_graph.add_edge(\n",
    "                relation.source_id,\n",
    "                relation.target_id,\n",
    "                relationship=relation.label,\n",
    "                description=relation.properties[\"relationship_description\"],\n",
    "            )\n",
    "        return nx_graph\n",
    "\n",
    "    def _collect_community_info(self, nx_graph, clusters):\n",
    "        \"\"\"Collect detailed information for each node based on their community.\"\"\"\n",
    "        community_mapping = {item.node: item.cluster for item in clusters}\n",
    "        community_info = {}\n",
    "        for item in clusters:\n",
    "            cluster_id = item.cluster\n",
    "            node = item.node\n",
    "            if cluster_id not in community_info:\n",
    "                community_info[cluster_id] = []\n",
    "\n",
    "            for neighbor in nx_graph.neighbors(node):\n",
    "                if community_mapping[neighbor] == cluster_id:\n",
    "                    edge_data = nx_graph.get_edge_data(node, neighbor)\n",
    "                    if edge_data:\n",
    "                        detail = f\"{node} -> {neighbor} -> {edge_data['relationship']} -> {edge_data['description']}\"\n",
    "                        community_info[cluster_id].append(detail)\n",
    "        return community_info\n",
    "\n",
    "    def _summarize_communities(self, community_info):\n",
    "        \"\"\"Generate and store summaries for each community.\"\"\"\n",
    "        for community_id, details in community_info.items():\n",
    "            details_text = (\n",
    "                \"\\n\".join(details) + \".\"\n",
    "            )  # Ensure it ends with a period\n",
    "            self.community_summary[\n",
    "                community_id\n",
    "            ] = self.generate_community_summary(details_text)\n",
    "\n",
    "    def get_community_summaries(self):\n",
    "        \"\"\"Returns the community summaries, building them if not already done.\"\"\"\n",
    "        if not self.community_summary:\n",
    "            self.build_communities()\n",
    "        return self.community_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text:  64%|██████▍   | 32/50 [02:49<01:52,  6.27s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6031067171095901 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BJfk33FRGf5jxB2bas1dHDrk on tokens per min (TPM): Limit 10000, Used 9085, Requested 1122. Please try again in 1.242s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Extracting paths from text:  70%|███████   | 35/50 [03:10<01:48,  7.22s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.04205088204457996 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BJfk33FRGf5jxB2bas1dHDrk on tokens per min (TPM): Limit 10000, Used 9802, Requested 1122. Please try again in 5.544s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Extracting paths from text:  78%|███████▊  | 39/50 [03:32<01:03,  5.78s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5407785364250153 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BJfk33FRGf5jxB2bas1dHDrk on tokens per min (TPM): Limit 10000, Used 9519, Requested 719. Please try again in 1.428s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Extracting paths from text:  80%|████████  | 40/50 [03:37<00:57,  5.72s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7006867370307215 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BJfk33FRGf5jxB2bas1dHDrk on tokens per min (TPM): Limit 10000, Used 9919, Requested 719. Please try again in 3.827s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Extracting paths from text:  86%|████████▌ | 43/50 [03:54<00:35,  5.06s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6271832592932969 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BJfk33FRGf5jxB2bas1dHDrk on tokens per min (TPM): Limit 10000, Used 9731, Requested 1008. Please try again in 4.434s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Extracting paths from text:  92%|█████████▏| 46/50 [04:14<00:23,  5.82s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.335784751143406 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BJfk33FRGf5jxB2bas1dHDrk on tokens per min (TPM): Limit 10000, Used 9859, Requested 1235. Please try again in 6.564s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Extracting paths from text: 100%|██████████| 50/50 [04:53<00:00,  5.86s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "index = PropertyGraphIndex(\n",
    "    nodes=nodes,\n",
    "    property_graph_store=GraphRAGStore(),\n",
    "    kg_extractors=[kg_extractor],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.property_graph_store.build_communities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.llms import LLM\n",
    "class GraphRAGQueryEngine(CustomQueryEngine):\n",
    "    graph_store: GraphRAGStore\n",
    "    llm: LLM\n",
    "\n",
    "    def custom_query(self, query_str: str) -> str:\n",
    "        \"\"\"Process all community summaries to generate answers to a specific query.\"\"\"\n",
    "        community_summaries = self.graph_store.get_community_summaries()\n",
    "        community_answers = [\n",
    "            self.generate_answer_from_summary(community_summary, query_str)\n",
    "            for _, community_summary in community_summaries.items()\n",
    "        ]\n",
    "\n",
    "        final_answer = self.aggregate_answers(community_answers)\n",
    "        return final_answer\n",
    "\n",
    "    def generate_answer_from_summary(self, community_summary, query):\n",
    "        \"\"\"Generate an answer from a community summary based on a given query using LLM.\"\"\"\n",
    "        prompt = (\n",
    "            f\"Given the community summary: {community_summary}, \"\n",
    "            f\"how would you answer the following query? Query: {query}\"\n",
    "        )\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=\"I need an answer based on the above information.\",\n",
    "            ),\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        cleaned_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return cleaned_response\n",
    "\n",
    "    def aggregate_answers(self, community_answers):\n",
    "        \"\"\"Aggregate individual community answers into a final, coherent response.\"\"\"\n",
    "        # intermediate_text = \" \".join(community_answers)\n",
    "        prompt = \"Combine the following intermediate answers into a final, concise response.\"\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=f\"Intermediate answers: {community_answers}\",\n",
    "            ),\n",
    "        ]\n",
    "        final_response = self.llm.chat(messages)\n",
    "        cleaned_final_response = re.sub(\n",
    "            r\"^assistant:\\s*\", \"\", str(final_response)\n",
    "        ).strip()\n",
    "        return cleaned_final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The only news related to the financial sector is that Nirmal Bang has given a Buy Rating to Tata Chemicals Ltd. (TTCH), indicating a positive investment recommendation. The rest of the provided information does not contain any news related to the financial sector."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = GraphRAGQueryEngine(\n",
    "    graph_store=index.property_graph_store, llm=llm\n",
    ")\n",
    "response = query_engine.query(\"What are news related to financial sector?\")\n",
    "display(Markdown(f\"{response.response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
